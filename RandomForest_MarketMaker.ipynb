{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d07226",
   "metadata": {},
   "source": [
    "# Use Kucoin Spot, test & train supervised data fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import os\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from finta import TA\n",
    "import traceback\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class TradingBot:\n",
    "    def __init__(self, symbol): #amount, take_profit_percentage, stop_loss_percentage):\n",
    "        load_dotenv()\n",
    "        self.symbol = symbol\n",
    "        self.amount = amount\n",
    "        self.take_profit_percentage = take_profit_percentage\n",
    "        self.stop_loss_percentage = stop_loss_percentage\n",
    "        self.exchange = ccxt.kucoin({\n",
    "            'apiKey': os.getenv('API_KEY'),\n",
    "            'secret': os.getenv('SECRET_KEY'),\n",
    "            'password': os.getenv('PASSPHRASE'),\n",
    "            'enableRateLimit': True\n",
    "        })\n",
    "        \n",
    "        # Initialize these attributes\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "\n",
    "    def calculate_atr(self, data, period=14):\n",
    "        df = pd.DataFrame(data, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\"])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        df['ATR'] = TA.ATR(df, period)\n",
    "\n",
    "        # Print ATR analysis\n",
    "        print(\"ATR Analysis:\")\n",
    "        print(\"--------------\")\n",
    "        print(\"Latest ATR value:\", df['ATR'].iloc[-1])\n",
    "        print(\"ATR Percentile Analysis:\")\n",
    "        print(\"Percentiles: 10% - 90%\")\n",
    "        print(df['ATR'].describe(percentiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))\n",
    "        print(\"=\" * 50)  # Add a separator\n",
    "        return df['ATR'].iloc[-1]\n",
    "    \n",
    "    def save_data_to_csv(self, data, filename):\n",
    "        header = [\"timestamp\", \"open\", \"high\", \"low\", \"close\"]\n",
    "        with open(filename, 'a', newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            if csv_file.tell() == 0:\n",
    "                writer.writerow(header)\n",
    "            for d in data:\n",
    "                writer.writerow(d)\n",
    "                \n",
    "    def fetch_order_book(self, symbol='BTC/USDT'):\n",
    "        try:\n",
    "            order_book = self.exchange.fetch_order_book(symbol)\n",
    "            bids = np.array(order_book['bids'])  # Convert to NumPy array\n",
    "            asks = np.array(order_book['asks'])  # Convert to NumPy array\n",
    "\n",
    "            return bids, asks\n",
    "\n",
    "        except ccxt.NetworkError as e:\n",
    "            self.handle_exception(f\"Network error: {e}\")\n",
    "        except ccxt.ExchangeError as e:\n",
    "            self.handle_exception(f\"Exchange error: {e}\")\n",
    "        except Exception as e:\n",
    "            self.handle_exception(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "    def calculate_best_bids_asks(self, bids, asks, num_levels=5):\n",
    "        # Sort bids and asks by price in descending order\n",
    "        sorted_bids = sorted(bids, key=lambda x: x[0], reverse=True)\n",
    "        sorted_asks = sorted(asks, key=lambda x: x[0])\n",
    "\n",
    "        # Select top num_levels bids and asks\n",
    "        best_bids = sorted_bids[:num_levels]\n",
    "        best_asks = sorted_asks[:num_levels]\n",
    "\n",
    "        return best_bids, best_asks\n",
    "\n",
    "    def fetch_data_and_preprocess(self, timeframe='15m', limit=1000):\n",
    "        try:\n",
    "            data = []\n",
    "            since = None\n",
    "            ohlcv_features = None  # Initialize ohlcv_features\n",
    "\n",
    "            while True:\n",
    "                ohlcv = self.exchange.fetch_ohlcv(self.symbol, timeframe, since=since, limit=limit)\n",
    "\n",
    "                if len(ohlcv) == 0:\n",
    "                    break\n",
    "                since = ohlcv[-1][0] + 1\n",
    "                data.extend(list(ohlcv_point) for ohlcv_point in ohlcv)\n",
    "\n",
    "                # Extract OHLCV features\n",
    "                ohlcv_features = np.array([d[0:6] for d in data if len(d) == 6])\n",
    "\n",
    "                # Fetch order book data\n",
    "                bids, asks = self.fetch_order_book(self.symbol)\n",
    "\n",
    "                # Calculate best bids and asks\n",
    "                best_bids, best_asks = self.calculate_best_bids_asks(bids, asks, num_levels=5)\n",
    "\n",
    "                # Convert lists to NumPy arrays\n",
    "                best_bids = np.array(best_bids)\n",
    "                best_asks = np.array(best_asks)\n",
    "                \n",
    "            \n",
    "                # Concatenate OHLCV and order book features\n",
    "                concatenated_features = np.concatenate((ohlcv_features[-5:, :], best_bids, best_asks), axis=1)\n",
    "                \n",
    "                print(f\"Shape of concatenated features: {concatenated_features.shape}\")\n",
    "\n",
    "                # Print the shapes\n",
    "                print(\"Shapes - ohlcv_features:\", ohlcv_features.shape)\n",
    "                print(\"Shapes - best_bids:\", best_bids.shape)\n",
    "                print(\"Shapes - best_asks:\", best_asks.shape)\n",
    "\n",
    "                print(\"=\" * 50)\n",
    "                print(f\"Number of data points before preprocessing: {len(data)}\")\n",
    "                \n",
    "                #print(\"Shape of features before modification:\", features.shape)\n",
    "\n",
    "                # Create the features array\n",
    "                features = np.array([d[0:6] for d in data if len(d) == 6])\n",
    "                \n",
    "                print(\"Shape of features after creation:\", features.shape)\n",
    "                print(\"Shape of features after modification:\", features.shape)\n",
    "\n",
    "                # Print the shape of features\n",
    "                print(f\"Shape of features: {features.shape}\")\n",
    "\n",
    "                for d in data:\n",
    "                    if len(d) != 6:\n",
    "                        print(\"Mismatched columns in data:\", d)\n",
    "\n",
    "                print(f\"Number of features before removing incomplete rows: {features.shape[0]}\")\n",
    "\n",
    "                if np.isnan(features).any():\n",
    "                    mask = ~np.isnan(features).any(axis=1)\n",
    "                    data = [d for d, m in zip(data, mask) if m]\n",
    "                    features = features[mask]\n",
    "\n",
    "                print(f\"Number of features after removing incomplete rows: {features.shape[0]}\")\n",
    "\n",
    "                scaler = MinMaxScaler()\n",
    "                scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "                print(f\"Mean values of scaled features: {np.mean(scaled_features, axis=0)}\")\n",
    "                print(f\"Standard deviation of scaled features: {np.std(scaled_features, axis=0)}\")\n",
    "                print(f\"Min values of scaled features: {np.min(scaled_features, axis=0)}\")\n",
    "                print(f\"Max values of scaled features: {np.max(scaled_features, axis=0)}\")\n",
    "                print(f\"Range of features: {np.max(scaled_features, axis=0) - np.min(scaled_features, axis=0)}\")\n",
    "\n",
    "                z_scores = np.abs(stats.zscore(scaled_features))\n",
    "                threshold = 3\n",
    "                outlier_mask = (z_scores < threshold).all(axis=1)\n",
    "\n",
    "                print(f\"Number of features after removing outliers: {np.sum(outlier_mask)}\")\n",
    "\n",
    "                data = [d for d, o in zip(data, outlier_mask) if o]\n",
    "                scaled_features = scaled_features[outlier_mask]\n",
    "                      \n",
    "                print(\"Before the error:\")\n",
    "                print(\"Shapes - ohlcv_features:\", ohlcv_features.shape)\n",
    "                print(\"Shapes - best_bids:\", best_bids.shape)\n",
    "                print(\"Shapes - best_asks:\", best_asks.shape)\n",
    "                print(\"Shape of concatenated features:\", concatenated_features.shape)\n",
    "                print(\"Shape of features:\", features.shape)\n",
    "\n",
    "\n",
    "                # Define the deterministic objective function\n",
    "                def deterministic_objective_function(d):\n",
    "                    return 1 if (len(d) == 6 and isinstance(d[4], (int, float)) and isinstance(d[1], (int, float)) and float(d[4]) < float(d[1])) else 0\n",
    "\n",
    "                # Generate the target using the deterministic objective function\n",
    "                target = np.array([deterministic_objective_function(d) for d in data])\n",
    "\n",
    "                self.save_data_to_csv(data, 'RF_btc.csv')  # Updated filename to reflect Random Forest\n",
    "                mean_values = np.mean(scaled_features, axis=0)\n",
    "                std_deviation = np.std(scaled_features, axis=0)\n",
    "                min_values = np.min(scaled_features, axis=0)\n",
    "                max_values = np.max(scaled_features, axis=0)\n",
    "                feature_ranges = max_values - min_values\n",
    "                for i in range(scaled_features.shape[1]):\n",
    "                    percentage_removed = (np.sum(~outlier_mask) / len(data)) * 100\n",
    "                    #print(f\"Percentage of Data Removed as Outliers: {percentage_removed:.2f}%\")\n",
    "                return data, scaled_features, scaler, target\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching and preprocessing data: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None, None\n",
    "\n",
    "    def train_models(self, features, target):\n",
    "        try:\n",
    "            # Print statements for checking the content and types of data\n",
    "             print(\"Features:\")\n",
    "             print(features[:5])  # Print the first 5 rows for inspection\n",
    "             print(\"Target:\")\n",
    "             print(target[:5])  # Print the first 5 elements for inspection\n",
    "\n",
    "            # Assign values to attributes\n",
    "            self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "            print(\"Shapes after train_test_split:\")\n",
    "            print(\"X_train:\", self.X_train.shape)\n",
    "            print(\"X_val:\", self.X_val.shape)\n",
    "            print(\"y_train:\", self.y_train.shape)\n",
    "            print(\"y_val:\", self.y_val.shape)\n",
    "\n",
    "            # Print the first few elements of X_train for inspection\n",
    "            print(\"X_train (first 10 rows):\", self.X_train[:10])\n",
    "\n",
    "            # Print the first few elements of X_val for inspection\n",
    "            print(\"X_val (first 10 rows):\", self.X_val[:10])\n",
    "\n",
    "            # Print the first few elements of y_train for inspection\n",
    "            print(\"y_train (first 10 elements):\", self.y_train[:10])\n",
    "\n",
    "            # Print the first few elements of X_val for inspection\n",
    "            print(\"y_val (first 10 rows):\", self.y_val[:10])\n",
    "\n",
    "            # Check if there are any string indices in X_train or X_val\n",
    "            print(\"Types in X_train:\", [type(val) for val in self.X_train.flatten()[:10]])\n",
    "            print(\"Types in X_val:\", [type(val) for val in self.X_val.flatten()[:10]])\n",
    "            print(\"Types in y_train:\", [type(val) for val in self.y_train.flatten()[:10]])\n",
    "            print(\"Types in y_val:\", [type(val) for val in self.y_val.flatten()[:10]])\n",
    "\n",
    "            # Create a random forest classifier\n",
    "            random_forest = RandomForestClassifier(random_state=42)\n",
    "        \n",
    "            \n",
    "            # Define a hyperparameter search space for RandomForestClassifier\n",
    "            param_space = {\n",
    "                'n_estimators': Integer(50, 200),\n",
    "                'max_depth': Integer(1, 30),\n",
    "                'min_samples_split': Integer(2, 10),\n",
    "                'min_samples_leaf': Integer(1, 4),\n",
    "                'criterion': Categorical(['gini', 'entropy']),\n",
    "                'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "            }\n",
    "\n",
    "            # Use BayesSearchCV for dynamic hyperparameter tuning\n",
    "            bayes_search = BayesSearchCV(random_forest, param_space, cv=5, n_iter=30, n_jobs=-1)            \n",
    "            bayes_search.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            # Get the best model\n",
    "            best_random_forest = bayes_search.best_estimator_\n",
    "\n",
    "            # Print the best hyperparameters\n",
    "            print(\"Best Hyperparameters:\", bayes_search.best_params_)\n",
    "\n",
    "            y_pred = best_random_forest.predict(self.X_val)\n",
    "\n",
    "            classification_rep = classification_report(self.y_val, y_pred)\n",
    "            confusion_mat = confusion_matrix(self.y_val, y_pred)\n",
    "\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_rep)\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(confusion_mat)\n",
    "            \n",
    "            accuracy_train = best_random_forest.score(self.X_train, self.y_train)  # Print accuracy on training set\n",
    "            print(f\"Accuracy on Training Set: {accuracy_train:.4f}\")\n",
    "            accuracy_val = best_random_forest.score(self.X_val, self.y_val)  # Print accuracy on validation set\n",
    "            print(f\"Accuracy on Validation Set: {accuracy_val:.4f}\")\n",
    "\n",
    "            # Parse the classification report\n",
    "            lines = classification_rep.split('\\n')\n",
    "            data = [line.split() for line in lines[2:-3]]  # Extracting relevant rows\n",
    "\n",
    "            # Ensure that each row has the expected structure\n",
    "            if data:\n",
    "                classes = [row[0] if len(row) > 0 else None for row in data]\n",
    "                precision = [float(row[1]) if len(row) > 1 and row[1] is not None else None for row in data]\n",
    "                recall = [float(row[2]) if len(row) > 2 and row[2] is not None else None for row in data]\n",
    "                f1 = [float(row[3]) if len(row) > 3 and row[3] is not None else None for row in data]\n",
    "                support = [int(row[4]) if len(row) > 4 else None for row in data]\n",
    "            else:\n",
    "                # Handle the case where data is empty\n",
    "                classes, precision, recall, f1, support = [], [], [], [], []\n",
    "                \n",
    "            if f1 and any(x is not None for x in f1):\n",
    "                weighted_avg_f1 = sum(x for x in f1 if x is not None) / len([x for x in f1 if x is not None])\n",
    "                print(f\"Weighted Average F1-Score: {weighted_avg_f1:.4f}\")\n",
    "            else:\n",
    "                print(\"No data for calculating F1 score.\")\n",
    "\n",
    "            # Check if there is data for precision, recall, and f1\n",
    "            if any(x is not None for x in precision):\n",
    "                weighted_avg_precision = sum(x for x in precision if x is not None) / len([x for x in precision if x is not None])\n",
    "                print(f\"Weighted Average Precision: {weighted_avg_precision:.4f}\")\n",
    "            else:\n",
    "                print(\"No data for calculating precision.\")\n",
    "\n",
    "            if any(x is not None for x in recall):\n",
    "                weighted_avg_recall = sum(x for x in recall if x is not None) / len([x for x in recall if x is not None])\n",
    "                print(f\"Weighted Average Recall: {weighted_avg_recall:.4f}\")\n",
    "            else:\n",
    "                print(\"No data for calculating recall.\")\n",
    "\n",
    "            precision = precision[-1]\n",
    "            recall = recall[-1]\n",
    "            f1 = f1[-1]\n",
    "\n",
    "            print(f\"Weighted Average Precision: {precision:.4f}\")\n",
    "            print(f\"Weighted Average Recall: {recall:.4f}\")\n",
    "            print(f\"Weighted Average F1-Score: {f1:.4f}\")\n",
    "\n",
    "            print(\"Random Forest Model Training Completed\")\n",
    "\n",
    "            print(\"Random Forest Model:\", best_random_forest)  # Print the models for debugging\n",
    "            with open('15mincheck15minochlvmodel.pkl', 'wb') as f:\n",
    "                pickle.dump(best_random_forest, f)\n",
    "            \n",
    "            return best_random_forest\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while training models: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        \n",
    "    def predict_market_direction(self, data, rf_model, scaler):\n",
    "        try:\n",
    "            features = np.array([d[0:6] for d in data if len(d) == 6])\n",
    "            if not features.any():\n",
    "                print(\"No valid data available for prediction.\")\n",
    "                return None\n",
    "            print(\"Shapes before scaling:\")\n",
    "            print(\"X_train:\", self.X_train.shape)\n",
    "            print(\"X_val:\", self.X_val.shape)\n",
    "            print(\"features:\", features.shape)\n",
    "\n",
    "            scaled_features = scaler.transform(features)\n",
    "\n",
    "            print(\"Shapes after scaling:\")\n",
    "            print(\"X_train:\", self.X_train.shape)\n",
    "            print(\"X_val:\", self.X_val.shape)\n",
    "            print(\"scaled_features:\", scaled_features.shape)\n",
    "\n",
    "            rf_accuracy_train = rf_model.score(self.X_train, self.y_train)\n",
    "            rf_accuracy_val = rf_model.score(self.X_val, self.y_val)\n",
    "            print(f\"Accuracy of Random Forest Model on Training Set: {rf_accuracy_train:.4f}\")\n",
    "            print(f\"Accuracy of Random Forest Model on Validation Set: {rf_accuracy_val:.4f}\")\n",
    "            \n",
    "            rf_prediction = rf_model.predict(scaled_features)\n",
    "            print(\"Random Forest Model Prediction on Validation Set:\")\n",
    "            print(rf_prediction)\n",
    "            # Ensure rf_prediction has the expected length\n",
    "            print(\"Length of rf_prediction:\", len(rf_prediction))\n",
    "            \n",
    "            final_prediction = rf_prediction[-1]\n",
    "            print(\"Final Prediction (0 for Bullish, 1 for Bearish):\")\n",
    "            print(final_prediction)\n",
    "            return final_prediction\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while predicting market direction: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_order_with_percentage_levels(self, side, entry_price):\n",
    "        try:\n",
    "            take_profit_price = entry_price * (1 + self.take_profit_percentage / 100)\n",
    "            stop_loss_price = entry_price * (1 - self.stop_loss_percentage / 100)\n",
    "            main_order = self.exchange.create_order(\n",
    "                self.symbol,\n",
    "                type='limit',\n",
    "                side=side,\n",
    "                amount=self.amount,\n",
    "                price=entry_price,\n",
    "                params={\n",
    "                    'postOnly': True,\n",
    "                    'timeInForce': 'GTC',\n",
    "                    'leverage': self.leverage\n",
    "                }\n",
    "            )\n",
    "            print(\"Main Order Created:\", main_order)\n",
    "            stop_loss_order = self.exchange.create_order(\n",
    "                self.symbol,\n",
    "                type='limit',\n",
    "                side='sell' if side == 'buy' else 'buy',\n",
    "                amount=self.amount,\n",
    "                price=stop_loss_price\n",
    "            )\n",
    "            print(\"Stop-Loss Order Created:\", stop_loss_order)\n",
    "            take_profit_order = self.exchange.create_order(\n",
    "                self.symbol,\n",
    "                type='limit',\n",
    "                side='sell' if side == 'buy' else 'buy',\n",
    "                amount=self.amount,\n",
    "                price=take_profit_price\n",
    "            )\n",
    "            print(\"Take-Profit Order Created:\", take_profit_order)\n",
    "            return main_order, stop_loss_order, take_profit_order\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating orders with percentage-based levels: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def create_limit_order(self, side, entry_price, trading_amount):\n",
    "        try:\n",
    "            # Create limit order\n",
    "            main_order = self.exchange.create_order(\n",
    "                self.symbol,\n",
    "                type='limit',\n",
    "                side=side,\n",
    "                amount=trading_amount,\n",
    "                price=entry_price,\n",
    "                params={\n",
    "                    'postOnly': False,\n",
    "                    'timeInForce': 'GTC',\n",
    "                    # You can add more parameters as needed\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Fetch order details for confirmation\n",
    "            order_details = self.exchange.fetch_order(main_order['id'])\n",
    "\n",
    "            # Log order details\n",
    "            self.logger.info(f\"Main Order Created: {main_order}\")\n",
    "            self.logger.info(f\"Order Details: {order_details}\")\n",
    "\n",
    "        except ccxt.NetworkError as e:\n",
    "            self.logger.error(f\"Network error: {e}\")\n",
    "        except ccxt.ExchangeError as e:\n",
    "            self.logger.error(f\"Exchange error: {e}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred: {e}\")\n",
    "\n",
    "    def calculate_midpoint_entry(self, best_bid, best_ask):\n",
    "        midpoint = (best_bid + best_ask) / 2.0\n",
    "        return midpoint\n",
    "\n",
    "\n",
    "    def main_trading_loop(self):\n",
    "        rf_model = None  # Initialize rf_model outside the loop\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                loop_start_time = datetime.now()\n",
    "\n",
    "                while True:\n",
    "                    current_time = datetime.now()\n",
    "                    elapsed_time = current_time - loop_start_time\n",
    "\n",
    "                    if elapsed_time.seconds < 10:\n",
    "                        print(f\"Waiting for {10 - elapsed_time.seconds} seconds before creating orders...\")\n",
    "                        sleep(10)\n",
    "\n",
    "                    data, scaled_features, scaler, target = self.fetch_data_and_preprocess()\n",
    "                    \n",
    "                    # Calculate ATR\n",
    "                    atr_value = self.calculate_atr(data)\n",
    "                \n",
    "\n",
    "                    # Train the random forest model\n",
    "                    rf_model = self.train_models(scaled_features, target)\n",
    "                    \n",
    "                    print(\"Shape of features in the main trading loop:\", scaled_features.shape)\n",
    "\n",
    "                    if rf_model is not None:\n",
    "                        print(\"Random Forest model was successfully trained.\")\n",
    "                        ticker = self.exchange.fetch_ticker(self.symbol)\n",
    "                        bid, ask = ticker['bid'], ticker['ask']\n",
    "                        midpoint = (bid + ask) / 2\n",
    "                        current_time = datetime.now()\n",
    "                        market_direction = self.predict_market_direction(data, rf_model, scaler)\n",
    "                        if market_direction is not None:\n",
    "                            print(\"The market is ---> {}\".format(market_direction))\n",
    "                            print(current_time.strftime(\"%B %d, %Y %I:%M %p\"))\n",
    "                            print(\"=\" * 50)\n",
    "\n",
    "                            if market_direction == 0:  # Bullish\n",
    "                                suggested_limit_price = midpoint - 0.01\n",
    "                            else:  # Bearish\n",
    "                                suggested_limit_price = midpoint + 0.01\n",
    "                                \n",
    "                            # Use atr_value as needed in your logic\n",
    "                            print(\"ATR Value:\", atr_value)\n",
    "\n",
    "                            Creating orders with a single level\n",
    "                            self.create_order_with_percentage_levels('buy' if market_direction == 0 else 'sell', suggested_limit_price)\n",
    "\n",
    "                            tp_price = suggested_limit_price * (1 + self.take_profit_percentage / 100)\n",
    "                            sl_price = suggested_limit_price * (1 - self.stop_loss_percentage / 100)\n",
    "                            print(f\"Take-Profit Price: {tp_price}, Stop-Loss Price: {sl_price}\")\n",
    "                            print(\"\\n\" + \"-\" * 50)\n",
    "                            start_time = current_time\n",
    "                        else:\n",
    "                            print(\"Error: Prediction failed.\")\n",
    "                            print(\"=\" * 50)\n",
    "                            sleep(30)\n",
    "                    else:\n",
    "                        print(\"Error: Training the random forest model failed.\")\n",
    "                        sleep(30)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred in the main trading loop: {e}\")\n",
    "                sleep(30)\n",
    "                break  # Exit the loop after an exception\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trading_bot = TradingBot(symbol='BTC/USDT')\n",
    "        amount=0.1,\n",
    "        take_profit_percentage=1.35,\n",
    "        stop_loss_percentage=1.35\n",
    "    \n",
    "    trading_bot.main_trading_loop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
